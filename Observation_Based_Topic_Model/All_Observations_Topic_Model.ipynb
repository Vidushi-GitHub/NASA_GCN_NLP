{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronitagarwala01/NASA_GCN_NLP/blob/main/Observation_Based_Topic_Model/All_Observations_Topic_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwJAAp0A27Ay"
      },
      "source": [
        "#**Topic Modelling Pipeline for the NASA GCN Platform with Pre-Defined Observation-Based Topic Labels**\n",
        "The aim of this project is to leverage the power of BERTopic to build a transformer powered topic model for the NASA GCN circular database.\n",
        "\n",
        "It is recommended to run this notebook in Google Colab.\n",
        "\n",
        "To get results consistent with those reported in the paper, it is recommended to run each section in this notebook sequentially and only once. Re-running it without restarting the runtime may yield slightly different results in certain sections due to the inherently stochastic nature of some of the agorithms used. We also recommend using Google Colab's L4 GPU runtime, which is what we used in our work. This is not a requirement and any GPU runtime may suffice, however using a different runtime may once again produce minor deviations from the results in our paper, as variations in hardware configurations can affect the outcomes of some of the algorithms used. Using the L4 GPU, however, may require the purchase of a temporary subscription to Colab Pro for $10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5edOPX82Nyc7"
      },
      "source": [
        "#STEP 1: Download And Unzip Necessary Files From Our Github Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKgxLYvxt90i"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/ronitagarwala01/NASA_GCN_NLP/raw/main/data/archive_2025.json.tar.gz -O /content/archive_2025.json.tar.gz\n",
        "!wget https://raw.githubusercontent.com/ronitagarwala01/NASA_GCN_NLP/main/data/custom_stopwords.txt -O /content/custom_stopwords.txt\n",
        "!wget https://github.com/ronitagarwala01/NASA_GCN_NLP/raw/main/Fine_Tuned_Models/tuned_mini_lm_1.zip -O /content/tuned_mini_lm_1.zip\n",
        "!wget https://github.com/ronitagarwala01/NASA_GCN_NLP/raw/main/Fine_Tuned_Models/tuned_mini_lm_2.zip -O /content/tuned_mini_lm_2.zip\n",
        "!wget https://github.com/ronitagarwala01/NASA_GCN_NLP/raw/main/Fine_Tuned_Models/tuned_mini_lm_3.zip -O /content/tuned_mini_lm_3.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/tuned_mini_lm_1.zip -d /content/tuned_mini_lm_1\n",
        "!mv /content/tuned_mini_lm_1/tuned_mini_lm_1/* /content/tuned_mini_lm_1\n",
        "!rm -rf /content/tuned_mini_lm_1/tuned_mini_lm_1\n",
        "\n",
        "!unzip /content/tuned_mini_lm_2.zip -d /content/tuned_mini_lm_2\n",
        "!mv /content/tuned_mini_lm_2/tuned_mini_lm_2/* /content/tuned_mini_lm_2\n",
        "!rm -rf /content/tuned_mini_lm_2/tuned_mini\n",
        "\n",
        "!unzip /content/tuned_mini_lm_3.zip -d /content/tuned_mini_lm_3\n",
        "!mv /content/tuned_mini_lm_3/tuned_mini_lm_3/* /content/tuned_mini_lm_3\n",
        "!rm -rf /content/tuned_mini_lm_3/tuned_mini_lm_3"
      ],
      "metadata": {
        "id": "t1FeGOmWe5c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Install Necessary Libraries"
      ],
      "metadata": {
        "id": "dQHTzlMLn3wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install bertopic==0.16.2 -qqq"
      ],
      "metadata": {
        "id": "F-3bEceVaRCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXSkpOy36e7"
      },
      "source": [
        "#Step 3: Extract GCN Circulars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4LBIsnqvN26"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Extract circular JSONs from tar file.\n",
        "'''\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "with tarfile.open('./archive_2025.json.tar.gz', 'r') as file:\n",
        "  file.extractall(path='./all_gcn_circulars')\n",
        "\n",
        "dir = os.listdir('./all_gcn_circulars/archive.json') # Store all file names as strings in dir\n",
        "\n",
        "# Add file path to beginning of file names in dir\n",
        "dir = ['./all_gcn_circulars/archive.json/' + filename for filename in sorted(dir)]\n",
        "\n",
        "print(f'Number of Circular JSONs: {len(dir)}\\n')\n",
        "print(f'First JSON path is: {dir[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRFBTZvpAOyX"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Extract circular bodies from JSON list.\n",
        "'''\n",
        "import json\n",
        "\n",
        "circulars = []\n",
        "circular_bodies = []\n",
        "time_stamps = []\n",
        "for file in dir:\n",
        "  with open(file, encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "    circulars.append(data)\n",
        "    circular_bodies.append(data[\"subject\"]+data[\"body\"])\n",
        "    time_stamps.append(data[\"createdOn\"])\n",
        "\n",
        "print(f'The first circular is:\\n {circulars[0]}\\n')\n",
        "print(f'The first circular body is:\\n {circular_bodies[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SZ7_xqgH8BG"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Remove all undefined characters.\n",
        "'''\n",
        "\n",
        "clean_texts = []\n",
        "for text in circular_bodies:\n",
        "  clean_text = text.replace('ï¿½', '')\n",
        "  clean_texts.append(clean_text)\n",
        "\n",
        "circular_bodies = clean_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqKXDs1FqgBb"
      },
      "source": [
        "#Step 4: Generate Custom Stopwords List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IJJcomqqPGP"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We will remove common English stopwords, punctuations, numbers, emails, and urls for preliminary statistical analysis and topic representations.\n",
        "We will also remove a hand-selected list of stopwords that do not add any value to our topics.\n",
        "As BERTopic uses a transformer based embedding model, it requires stopwords to build accurate embeddings.\n",
        "So removing stopwords before this step is unadvised.\n",
        "However, we can remove stopwords after embedding and clustering.\n",
        "We will use sklearn's WordVectorizer for this.\n",
        "'''\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords') # NLTK package for stopwords list\n",
        "\n",
        "new_stop_words = []\n",
        "new_stop_words = stopwords.words('english') # NLTK standard list of stopwords\n",
        "punctuation_list = list(string.punctuation) # Standard list of punctuations\n",
        "new_stop_words.extend(punctuation_list)\n",
        "\n",
        "# Get list of numbers and urls in circulars\n",
        "num_list = []\n",
        "url_list = []\n",
        "http_regex = re.compile(r\"http.*\")\n",
        "\n",
        "for text in circular_bodies:\n",
        "  word_list = text.split()\n",
        "\n",
        "  for word in word_list:\n",
        "    try:\n",
        "      float(word) # Check if word is numeric. Throws Value Error otherwise\n",
        "      num_list.append(word)\n",
        "    except ValueError:\n",
        "      pass\n",
        "\n",
        "    if re.match(http_regex, word): # Check if word begins with http\n",
        "      url_list.append(word)\n",
        "\n",
        "# Get list of emails\n",
        "email_list=[]\n",
        "for circular in circulars:\n",
        "  if \"email\" in circular:\n",
        "    email_list.append(circular[\"email\"])\n",
        "\n",
        "num_list = list(set(num_list)) # Remove duplicates\n",
        "new_stop_words.extend(num_list)\n",
        "\n",
        "url_list = list(set(url_list))\n",
        "new_stop_words.extend(url_list)\n",
        "\n",
        "email_list = list(set(email_list))\n",
        "new_stop_words.extend(email_list)\n",
        "\n",
        "with open('custom_stopwords.txt') as f:\n",
        "  for word in f:\n",
        "    new_stop_words.append(word.lower().strip())\n",
        "\n",
        "vectorizer_model = CountVectorizer(stop_words=new_stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nwZhk1A8OKt"
      },
      "source": [
        "#Step 5: Preliminary Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUAt4gbh8HPa"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Get word count distribution over all circulars.\n",
        "'''\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "sns.set_theme()\n",
        "\n",
        "counts=[]\n",
        "for text in circular_bodies:\n",
        "  counts.append(len(text.split()))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(counts, range=(0,1000), bins=100, color=sns.color_palette(\"Set2\", 1))\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Word Counts of Circular Bodies\")\n",
        "plt.xticks(np.arange(0, 1001, 100))\n",
        "plt.show()\n",
        "\n",
        "counts_over_1000 = [count>1000 for count in counts]\n",
        "print(f'Number of circulars with > 1000 words: {sum(counts_over_1000)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McK57eQtbsZa"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create word cloud over all GCN circulars.\n",
        "Includes bigrams and trigrams of words.\n",
        "'''\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "word_cloud = WordCloud(\n",
        "    collocations = True,\n",
        "    background_color = 'white',\n",
        "    max_words=100,\n",
        "    width=800,\n",
        "    height=600,\n",
        "    stopwords=new_stop_words).generate(' '.join(circular_bodies))\n",
        "\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI6DdD6qMtd4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Print the list of words in the word cloud\n",
        "'''\n",
        "\n",
        "word_frequencies = word_cloud.words_\n",
        "words = list(word_frequencies.keys())\n",
        "for word in words:\n",
        "  print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phnVmCY1HoBn"
      },
      "source": [
        "#Step 6: Embed GCN Circulars With The Default all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdyjnshP6nD_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Embed our circulars using the base all-MiniLM-L6-v2 model.\n",
        "'''\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Default model. Really fast, but only has context window of 256 tokens\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "minilm_l6_embeddings = model.encode(circular_bodies, show_progress_bar=True)\n",
        "np.save('minilm_l6_embeddings.npy', minilm_l6_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6KzPJs-9QOm"
      },
      "source": [
        "#Step 7: Embedding Model Evaluation For Observation-Based Circular Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o0gq8cPp45I"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We will now test our topic models on a small pre-defined dataset.\n",
        "We have gathered GCN circulars belonging to the following categories:\n",
        "  -High-Energy Observations\n",
        "  -Optical Observations\n",
        "  -Radio\n",
        "  -Neutrino\n",
        "  -Gravitational Wave\n",
        "In total we have 200 circulars in our dataset.\n",
        "Our goal is to find the topic model that can correctly classify most of these circulars based on Zero-Shot Classification.\n",
        "'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare dataset\n",
        "high_energy = [25672, 26248, 14327, 14131, 6000, 29383, 17260, 10699, 19545, 17866, 9117, 8860, 6212, 26146, 7009, 7744, 10283, 22271, 2462, 28289, 25300, 24996, 3348, 26573, 11527, 20928, 29800, 28011, 12555, 32813, 23226, 33757, 2505, 23140, 13558, 12449, 23445, 22699, 13959, 22115]\n",
        "optical = [28681, 22275, 21480, 3791, 5286, 2192, 31976, 23411, 18503, 23516, 31426, 31356, 33293, 17415, 31015, 598, 4738, 9761, 17736, 9628, 28161, 13557, 34651, 5615, 5065, 11329, 19277, 30221, 21931, 13710, 28226, 9179, 24256, 31394, 33941, 33098, 34473, 29604, 30112, 3199]\n",
        "radio = [9047, 13903, 30542, 23753, 21933, 21395, 20787, 19610, 18548, 18345, 21058, 21057, 20981, 20105, 20453, 20313, 20344, 20328, 18978, 19310, 19607, 19848, 20116, 19606, 18454, 19966, 18620, 20428, 17178, 17284, 17104, 16593, 15783, 14863, 13813, 13743, 16603, 16295, 15395, 10019]\n",
        "neutrino = [23605, 34933, 34838, 34798, 34163, 34166, 33876, 33589, 33430, 33850, 33128, 33122, 33097, 33040, 32741, 32523, 32499, 31681, 30468, 31128, 31476, 26718, 27642, 26812, 26716, 26707, 26623, 26620, 26189, 26181, 26049, 25836, 25842, 25693, 25225, 25210, 24865, 24854, 24573, 24410]\n",
        "gw = [34895, 34857, 34801, 34799, 34775, 34692, 34728, 34605, 34337, 34264, 34206, 34173, 34968, 27358, 27278, 27193, 27042, 26906, 26675, 26543, 26513, 26641, 26413, 26334, 26303, 26288, 26254, 26250, 26182, 25883, 25876, 25871, 25829, 25814, 25707, 25554, 25442, 25324, 25296, 25012]\n",
        "\n",
        "high_energy_train, high_energy_test = train_test_split(high_energy, test_size=0.2, random_state=0)\n",
        "optical_train, optical_test = train_test_split(optical, test_size=0.2, random_state=0)\n",
        "radio_train, radio_test = train_test_split(radio, test_size=0.2, random_state=0)\n",
        "neutrino_train, neutrino_test = train_test_split(neutrino, test_size=0.2, random_state=0)\n",
        "gw_train, gw_test = train_test_split(gw, test_size=0.2, random_state=0)\n",
        "\n",
        "train_observation_labels = []\n",
        "test_observation_labels = []\n",
        "\n",
        "# Get Training Set\n",
        "for circular in circulars:\n",
        "  if circular[\"circularId\"] in high_energy_train:\n",
        "    train_observation_labels.append(\"High Energy Observations\")\n",
        "  elif circular[\"circularId\"] in optical_train:\n",
        "    train_observation_labels.append(\"Optical Observations\")\n",
        "  elif circular[\"circularId\"] in radio_train:\n",
        "    train_observation_labels.append(\"Radio Observations\")\n",
        "  elif circular[\"circularId\"] in neutrino_train:\n",
        "    train_observation_labels.append(\"Neutrinos\")\n",
        "  elif circular[\"circularId\"] in gw_train:\n",
        "    train_observation_labels.append(\"Gravitational Wave\")\n",
        "  else:\n",
        "    train_observation_labels.append(\"\")\n",
        "\n",
        "# Get Test Set\n",
        "for circular in circulars:\n",
        "  if circular[\"circularId\"] in high_energy_test:\n",
        "    test_observation_labels.append(\"High Energy Observations\")\n",
        "  elif circular[\"circularId\"] in optical_test:\n",
        "    test_observation_labels.append(\"Optical Observations\")\n",
        "  elif circular[\"circularId\"] in radio_test:\n",
        "    test_observation_labels.append(\"Radio Observations\")\n",
        "  elif circular[\"circularId\"] in neutrino_test:\n",
        "    test_observation_labels.append(\"Neutrinos\")\n",
        "  elif circular[\"circularId\"] in gw_test:\n",
        "    test_observation_labels.append(\"Gravitational Wave\")\n",
        "  else:\n",
        "    test_observation_labels.append(\"\")\n",
        "\n",
        "print(len(train_observation_labels))\n",
        "print(len(test_observation_labels))\n",
        "print(len(high_energy_train))\n",
        "print(len(high_energy_test))\n",
        "print(len(optical_train))\n",
        "print(len(optical_test))\n",
        "print(len(radio_train))\n",
        "print(len(radio_test))\n",
        "print(len(neutrino_train))\n",
        "print(len(neutrino_test))\n",
        "print(len(gw_train))\n",
        "print(len(gw_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6Rfg6igdgJq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We will perform Zero-Shot Topic Modelling to match circulars to pre-defined cadidate labels.\n",
        "We'll be using a cosine similarity with various thresholds to match topic labels to circulars.\n",
        "For the circulars that don't match any of the labels above our threshold we perform regular Topic Modelling.\n",
        "Finally, we calculate accuracy scores for each embedding model and cosine threshold based on our dataset.\n",
        "'''\n",
        "import pandas as pd\n",
        "from umap import UMAP\n",
        "\n",
        "embeddings_list = [\"Base Model\", \"Epoch 1\", \"Epoch 2\", \"Epoch 3\"]\n",
        "accuracy_scores = pd.DataFrame(index=[\"Train\", \"Test\"], columns=embeddings_list)\n",
        "\n",
        "umap_model = UMAP(n_neighbors=15,\n",
        "                  n_components=5,\n",
        "                  min_dist=0.0,\n",
        "                  metric='cosine',\n",
        "                  low_memory=False,\n",
        "                  random_state=0)\n",
        "\n",
        "candidate_topics = [\"High Energy Observations\",\n",
        "                    \"Optical Observations\",\n",
        "                    \"Radio Observations\",\n",
        "                    \"Neutrinos\",\n",
        "                    \"Gravitational Wave\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69x3A2NY9QOq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Compute accuracy for all-MiniLM-L6-v2 embeddings on training set\n",
        "'''\n",
        "from bertopic import BERTopic\n",
        "\n",
        "vectorizer_model = CountVectorizer()\n",
        "embeddings = np.load(\"minilm_l6_embeddings.npy\")\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "error_count=0\n",
        "\n",
        "# Compute accuracy scores based on number of matches between candidate labels and topic model labels\n",
        "topic_model = BERTopic(verbose=True,\n",
        "                       umap_model=umap_model,\n",
        "                       vectorizer_model=vectorizer_model,\n",
        "                       zeroshot_topic_list=candidate_topics,\n",
        "                       zeroshot_min_similarity=0.1, # Assign topic to each circular if it crosses threshold\n",
        "                       embedding_model=model)\n",
        "\n",
        "topics, probs = topic_model.fit_transform(circular_bodies, embeddings)\n",
        "\n",
        "topic_labels=[]\n",
        "for topic in topics:\n",
        "  topic_labels.append(topic_model.topic_labels_[topic])\n",
        "\n",
        "score = 0\n",
        "for i, label in enumerate(train_observation_labels):\n",
        "  if label != \"\":\n",
        "    if topic_labels[i] == label:\n",
        "      score += 1\n",
        "    else:\n",
        "      error_count += 1\n",
        "\n",
        "print(f\"Raw Score: {score}\")\n",
        "print(f\"Raw Error: {error_count}\")\n",
        "accuracy = (score / (score + error_count)) * 100\n",
        "accuracy_scores.at[\"Train\", \"Base Model\"] = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruuAmQUa9QOq"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "Compute accuracy scores for base model on test set\n",
        "'''\n",
        "\n",
        "error_count=0\n",
        "\n",
        "topic_model = BERTopic(verbose=True,\n",
        "                      umap_model=umap_model,\n",
        "                      vectorizer_model=vectorizer_model,\n",
        "                      zeroshot_topic_list=candidate_topics,\n",
        "                      zeroshot_min_similarity=0.1,\n",
        "                      embedding_model=model)\n",
        "\n",
        "topics, probs = topic_model.fit_transform(circular_bodies, embeddings)\n",
        "\n",
        "topic_labels=[]\n",
        "for topic in topics:\n",
        "  topic_labels.append(topic_model.topic_labels_[topic])\n",
        "\n",
        "score = 0\n",
        "for i, label in enumerate(test_observation_labels):\n",
        "  if label != \"\":\n",
        "    if topic_labels[i] == label:\n",
        "      score += 1\n",
        "    else:\n",
        "      error_count += 1\n",
        "\n",
        "print(f\"Raw Score: {score}\")\n",
        "print(f\"Raw Error: {error_count}\")\n",
        "accuracy = (score / (score + error_count)) * 100\n",
        "accuracy_scores.at[\"Test\", \"Base Model\"] = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCva6ZXO9QOq"
      },
      "outputs": [],
      "source": [
        "accuracy_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7nV7MV79vs_"
      },
      "source": [
        "#Step 8: Contrastive Fine-Tuning On Labelled Dataset (Do Not Run If You're Not Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7WsHYuv9vs_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We will now fine-tune our sentence embedder model using Contrastive Loss.\n",
        "The goal is to fine-tune our embeddings so that circulars belong to similar topics are embedded more closely,\n",
        "while circulars belonging to dissimilar topics are embedded far away in the vector space.\n",
        "Theoretically this should help improve our zero-shot topic modelling accuracy.\n",
        "We'll be using the same Gravitational Wave + Counterpart circular dataset as before.\n",
        "'''\n",
        "\n",
        "# Prepare Dataset\n",
        "event_dataset = []\n",
        "for text, label in zip(circular_bodies, train_observation_labels):\n",
        "  example = {}\n",
        "  if label != \"\":\n",
        "    example[\"text\"] = text\n",
        "    example[\"label\"] = candidate_topics.index(label)\n",
        "    event_dataset.append(example)\n",
        "\n",
        "# Add label names to event dataset as well\n",
        "for label in candidate_topics:\n",
        "  example = {}\n",
        "  example[\"text\"] = label\n",
        "  example[\"label\"] = candidate_topics.index(label)\n",
        "  event_dataset.append(example)\n",
        "\n",
        "print(len(event_dataset))\n",
        "unique_values = {d[\"label\"] for d in event_dataset if \"label\" in d}\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i2ASofU9vtA"
      },
      "outputs": [],
      "source": [
        "# Prepare Similar Event Dataset\n",
        "event_pair_dataset = []\n",
        "for i in event_dataset:\n",
        "  for j in event_dataset:\n",
        "    # if i == j:\n",
        "    #   continue\n",
        "    example={}\n",
        "    example[\"texts\"] = [i[\"text\"], j[\"text\"]]\n",
        "    if i[\"label\"] == j[\"label\"]:\n",
        "      example[\"label\"] = 1\n",
        "    else:\n",
        "      example[\"label\"] = 0\n",
        "    event_pair_dataset.append(example)\n",
        "\n",
        "print(len(event_pair_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_Zjm5Kt9vtA"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Train for 1 epoch on the training set\n",
        "'''\n",
        "import torch\n",
        "import random\n",
        "from sentence_transformers.readers import InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import losses\n",
        "\n",
        "# Set seeds to help reproducibility\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "seed_value = 42\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# Prepare Training Examples, Loss, and Model (all-MiniLM-L6-v2)\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "train_examples = [InputExample(texts=example[\"texts\"], label=example[\"label\"]) for example in event_pair_dataset]\n",
        "\n",
        "# Prepare DataLoader Object\n",
        "train_dataloader = DataLoader(train_examples,\n",
        "                              shuffle=True,\n",
        "                              batch_size=1,\n",
        "                              num_workers=0)\n",
        "train_size = len(train_dataloader)\n",
        "\n",
        "# Use Contrastive Training Loss\n",
        "train_loss = losses.ContrastiveLoss(model=model)\n",
        "\n",
        "# Tune the model\n",
        "model.old_fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          epochs=1,\n",
        "          warmup_steps=100)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"tuned_mini_lm_1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Train for 2 epochs on the training set\n",
        "'''\n",
        "\n",
        "# Set seeds to help reproducibility\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "seed_value = 42\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# Prepare Training Examples, Loss, and Model (all-MiniLM-L6-v2)\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "train_examples = [InputExample(texts=example[\"texts\"], label=example[\"label\"]) for example in event_pair_dataset]\n",
        "\n",
        "# Prepare DataLoader Object\n",
        "train_dataloader = DataLoader(train_examples,\n",
        "                              shuffle=True,\n",
        "                              batch_size=1,\n",
        "                              num_workers=0)\n",
        "train_size = len(train_dataloader)\n",
        "\n",
        "# Use Contrastive Training Loss\n",
        "train_loss = losses.ContrastiveLoss(model=model)\n",
        "\n",
        "# Tune the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          epochs=2,\n",
        "          warmup_steps=100)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"tuned_mini_lm_2\")"
      ],
      "metadata": {
        "id": "rgWC1YUyR3pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Train for 3 epochs on the training set\n",
        "'''\n",
        "\n",
        "# Set seeds to help reproducibility\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "seed_value = 42\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# Prepare Training Examples, Loss, and Model (all-MiniLM-L6-v2)\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "train_examples = [InputExample(texts=example[\"texts\"], label=example[\"label\"]) for example in event_pair_dataset]\n",
        "\n",
        "# Prepare DataLoader Object\n",
        "train_dataloader = DataLoader(train_examples,\n",
        "                              shuffle=True,\n",
        "                              batch_size=1,\n",
        "                              num_workers=0)\n",
        "train_size = len(train_dataloader)\n",
        "\n",
        "# Use Contrastive Training Loss\n",
        "train_loss = losses.ContrastiveLoss(model=model)\n",
        "\n",
        "# Tune the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          epochs=3,\n",
        "          warmup_steps=100)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"tuned_mini_lm_3\")"
      ],
      "metadata": {
        "id": "jVCWNRaRR6jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFrpjsXv9vtB"
      },
      "outputs": [],
      "source": [
        "# Get new all-MiniLM-L6-v2 embeddings\n",
        "model_1 = SentenceTransformer(\"/content/tuned_mini_lm_1\")\n",
        "model_2 = SentenceTransformer(\"/content/tuned_mini_lm_2\")\n",
        "model_3 = SentenceTransformer(\"/content/tuned_mini_lm_3\")\n",
        "\n",
        "tuned_embeddings_1 = model_1.encode(circular_bodies, show_progress_bar=True)\n",
        "tuned_embeddings_2 = model_2.encode(circular_bodies, show_progress_bar=True)\n",
        "tuned_embeddings_3 = model_3.encode(circular_bodies, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lNvcxufUAIa"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Compute training accuracy for fine-tuned all-MiniLM-L6-v2 embeddings\n",
        "'''\n",
        "\n",
        "error_count=0\n",
        "\n",
        "# Compute accuracy scores based on number of matches between candidate labels and topic model labels\n",
        "for i in range(1,4):\n",
        "  embeddings = eval(f\"tuned_embeddings_{i}\")\n",
        "  model = eval(f\"model_{i}\")\n",
        "  topic_model = BERTopic(verbose=True,\n",
        "                         umap_model=umap_model,\n",
        "                         vectorizer_model=vectorizer_model,\n",
        "                         zeroshot_topic_list=candidate_topics,\n",
        "                         zeroshot_min_similarity=0.1,\n",
        "                         embedding_model=model)\n",
        "\n",
        "  topics, probs = topic_model.fit_transform(circular_bodies, embeddings)\n",
        "\n",
        "  topic_labels=[]\n",
        "  for topic in topics:\n",
        "    topic_labels.append(topic_model.topic_labels_[topic])\n",
        "\n",
        "  score = 0\n",
        "  error_count = 0\n",
        "  for j, label in enumerate(train_observation_labels):\n",
        "    if label != \"\":\n",
        "      if topic_labels[j] == label:\n",
        "        score += 1\n",
        "      else:\n",
        "        error_count += 1\n",
        "\n",
        "  print(f\"Raw Score for Epoch {i}: {score}\")\n",
        "  print(f\"Raw Error for Epoch {i}: {error_count}\")\n",
        "  accuracy = (score / (score + error_count)) * 100\n",
        "  epoch = f\"Epoch {i}\"\n",
        "  accuracy_scores.at[\"Train\", epoch] = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX5l0_0Z9vtC"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Compute test accuracy for fine-tuned all-MiniLM-L6-v2 embeddings\n",
        "'''\n",
        "\n",
        "error_count=0\n",
        "\n",
        "# Compute accuracy scores based on number of matches between candidate labels and topic model labels\n",
        "for i in range(1,4):\n",
        "  embeddings = eval(f\"tuned_embeddings_{i}\")\n",
        "  model = eval(f\"model_{i}\")\n",
        "  topic_model = BERTopic(verbose=True,\n",
        "                         umap_model=umap_model,\n",
        "                         vectorizer_model=vectorizer_model,\n",
        "                         zeroshot_topic_list=candidate_topics,\n",
        "                         zeroshot_min_similarity=0.1,\n",
        "                         embedding_model=model)\n",
        "\n",
        "  topics, probs = topic_model.fit_transform(circular_bodies, embeddings)\n",
        "\n",
        "  topic_labels=[]\n",
        "  for topic in topics:\n",
        "    topic_labels.append(topic_model.topic_labels_[topic])\n",
        "\n",
        "  score = 0\n",
        "  error_count = 0\n",
        "  for j, label in enumerate(test_observation_labels):\n",
        "    if label != \"\":\n",
        "      if topic_labels[j] == label:\n",
        "        score += 1\n",
        "      else:\n",
        "        error_count += 1\n",
        "\n",
        "  print(f\"Raw Score for Epoch {i}: {score}\")\n",
        "  print(f\"Raw Error for Epoch {i}: {error_count}\")\n",
        "  accuracy = (score / (score + error_count)) * 100\n",
        "  epoch = f\"Epoch {i}\"\n",
        "  accuracy_scores.at[\"Test\", epoch] = accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhapwGdZ9vtC"
      },
      "outputs": [],
      "source": [
        "accuracy_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5i6-FCX9vtC"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create latex table for accuracy scores\n",
        "'''\n",
        "\n",
        "accuracy_latex_table = accuracy_scores.to_latex(float_format=\"%.2f\")\n",
        "accuracy_latex_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDkn8jghCbYL"
      },
      "source": [
        "#Step 9: Zero-Shot Topic Modelling For Observation-Based Circular Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D67XaHptrOrx"
      },
      "outputs": [],
      "source": [
        "# Get new all-MiniLM-L6-v2 embeddings\n",
        "model_2 = SentenceTransformer(\"/content/tuned_mini_lm_2\")\n",
        "tuned_embeddings_2 = model_2.encode(circular_bodies, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmVNL2DbCbYM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We'll now perform topic modelling again but with Zero-Shot enabled.\n",
        "We will use our fine-tuned all-MiniLM-L6-v2 model for this which was tuned for 1 epoch on the gravitational wave dataset.\n",
        "We attempt to fit our topics into pre-defined candidate labels using zero-shot topic modelling.\n",
        "'''\n",
        "candidate_topics = [\"High Energy Observations\",\n",
        "                    \"Optical Observations\",\n",
        "                    \"Radio Observations\",\n",
        "                    \"Neutrinos\",\n",
        "                    \"Gravitational Wave\"]\n",
        "\n",
        "umap_model = UMAP(n_neighbors=15,\n",
        "                  n_components=5,\n",
        "                  min_dist=0.0,\n",
        "                  metric='cosine',\n",
        "                  low_memory=False,\n",
        "                  random_state=0)\n",
        "\n",
        "# Build Topic Model with BERTopic\n",
        "topic_model = BERTopic(verbose=True,\n",
        "                       embedding_model=model_2,\n",
        "                       umap_model=umap_model,\n",
        "                       min_topic_size=100,\n",
        "                       vectorizer_model=vectorizer_model,\n",
        "                       zeroshot_topic_list=candidate_topics,\n",
        "                       zeroshot_min_similarity=0.1)\n",
        "\n",
        "topics, probs = topic_model.fit_transform(circular_bodies, tuned_embeddings_2)\n",
        "topic_labels = [value for key, value in topic_model.topic_labels_.items()]\n",
        "topic_model.set_topic_labels(topic_labels)\n",
        "\n",
        "freq = topic_model.get_topic_info()\n",
        "freq.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4hO7im5U_y3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create observation-based topic csv file\n",
        "'''\n",
        "from datetime import datetime\n",
        "\n",
        "circular_topic_df = pd.DataFrame()\n",
        "circular_topic_df[\"Circular ID\"] = [item[\"circularId\"] for item in circulars]\n",
        "circular_topic_df[\"Subject\"] = [item[\"subject\"] for item in circulars]\n",
        "circular_topic_df[\"Date\"] = [datetime.utcfromtimestamp(item[\"createdOn\"]/1000) for item in circulars]\n",
        "circular_topic_df[\"Label\"] = [topic_model.topic_labels_[i] for i in topics]\n",
        "\n",
        "circular_topic_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BlRta11uWME2"
      },
      "outputs": [],
      "source": [
        "circular_topic_df.to_csv('observation_based_topics.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pde_l9YsCbYO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Apply TSNE to reduce the dimensionality of the embeddings and visualize the clusters.\n",
        "'''\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "reduced_embeddings = TSNE(n_components=2, n_jobs=1, random_state=0, verbose=2).fit_transform(tuned_embeddings_2)\n",
        "\n",
        "fig = go.Figure()\n",
        "tsne_df = pd.DataFrame()\n",
        "tsne_df[\"x\"], tsne_df[\"y\"] = reduced_embeddings[:, 0], reduced_embeddings[:, 1]\n",
        "tsne_df[\"topics\"] = [topic_model.topic_labels_[i] for i in topics]\n",
        "\n",
        "for label in list(topic_model.topic_labels_.values()):\n",
        "  sub_df = tsne_df.loc[tsne_df[\"topics\"] == label]\n",
        "  fig.add_trace(\n",
        "    go.Scattergl(\n",
        "      x=sub_df[\"x\"],\n",
        "      y=sub_df[\"y\"],\n",
        "      mode=\"markers\",\n",
        "      name=str(label[label.find('_')+1:]) + \" (\" + str(sub_df.shape[0]) + \")\",\n",
        "    )\n",
        "  )\n",
        "\n",
        "fig.update_traces(\n",
        "  marker=dict(\n",
        "    size=5,\n",
        "    opacity=0.5,\n",
        "  )\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "  title={\n",
        "    'text': \"<b>General Coordinates Network (GCN): Observation-Based Topic Clusters</b>\",\n",
        "    'x': 0.5,\n",
        "    'xanchor': 'center'\n",
        "  },\n",
        "  width=1200,\n",
        "  height=800,\n",
        "  legend_title_text=\"Topics (Circular Counts)\",\n",
        "  legend=dict(\n",
        "    x=1.05,\n",
        "    y=1,\n",
        "    traceorder='normal',\n",
        "    bgcolor='rgba(0,0,0,0)',\n",
        "    bordercolor='rgba(0,0,0,0)',\n",
        "    font=dict(size=16)\n",
        "  ),\n",
        "  xaxis=dict(\n",
        "    showticklabels=False\n",
        "  ),\n",
        "  yaxis=dict(\n",
        "    showticklabels=False\n",
        "  )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDvmPLplCbYO"
      },
      "outputs": [],
      "source": [
        "# Display a Similarity Matrix for all Topics\n",
        "topic_model.visualize_heatmap(width=950, height=650, custom_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkxBLtv9CbYO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create word cloud over our candidate topics\n",
        "'''\n",
        "\n",
        "fig, axs = plt.subplots(len(candidate_topics), 1, figsize=(10, 60))\n",
        "\n",
        "# Join all documents of a candidate topic together and generate word cloud\n",
        "for topic_num, ax in enumerate(axs):\n",
        "  word_cloud = WordCloud(\n",
        "        collocations=True,\n",
        "        background_color='white',\n",
        "        max_words=100,\n",
        "        width=1000,\n",
        "        height=800).generate(' '.join([text for i, text in enumerate(circular_bodies) if topic_model.topic_labels_[topics[i]] == candidate_topics[topic_num]]))\n",
        "\n",
        "  ax.imshow(word_cloud, interpolation='bilinear')\n",
        "  ax.set_title(candidate_topics[topic_num], fontsize=18, fontweight=\"bold\", y=1.05)\n",
        "  ax.axis(\"off\")\n",
        "\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fig.layout.colorway)"
      ],
      "metadata": {
        "id": "-d1wWnS9RIwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncXPS2PKCbYP"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We will now perform Trend Analysis over our Topic Clusters.\n",
        "'''\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "num_topics = len(freq)\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_style(\"ticks\")\n",
        "sns.set_context(\"paper\", font_scale=1.5)\n",
        "plt.figure(figsize=(18, 6))\n",
        "custom_colors = ['#023eff', '#e8000b', '#1ac938', '#8b2be2', '#ff7c00']\n",
        "custom_palette =  sns.color_palette(custom_colors)\n",
        "\n",
        "all_topic_dates = []\n",
        "all_dates = [] # List to store all dates\n",
        "\n",
        "# Iterate over all timestamps and check their respective document's topic\n",
        "for i, time_stamp in enumerate(time_stamps):\n",
        "  if time_stamp == 0: # Discard invalid dates\n",
        "      continue\n",
        "  date = datetime.utcfromtimestamp(time_stamp/1000)\n",
        "  topic = topic_model.topic_labels_[topics[i]]\n",
        "  all_topic_dates.append({'Date': date, 'Topic': topic})\n",
        "  all_dates.append(date)\n",
        "\n",
        "topic_dates_df = pd.DataFrame(all_topic_dates)\n",
        "my_bins = pd.date_range(start=min(all_dates), end=max(all_dates), freq='6M')\n",
        "sns.histplot(topic_dates_df,\n",
        "             x='Date',\n",
        "             hue='Topic',\n",
        "             multiple=\"stack\",\n",
        "             bins=mdates.date2num(my_bins),\n",
        "             alpha=0.75,\n",
        "             linewidth=0.2,\n",
        "             palette=custom_palette)\n",
        "\n",
        "# Calculate the number of 6-month intervals between start and end dates\n",
        "start_date = min(all_dates)\n",
        "end_date = max(all_dates)\n",
        "num_intervals = (end_date.year - start_date.year) * 2 + (end_date.month - start_date.month) // 6\n",
        "\n",
        "# Calculate the adjusted end date based on the number of intervals\n",
        "adjusted_end_date = start_date + pd.DateOffset(months=num_intervals * 6)\n",
        "\n",
        "plt.xlim(start_date, adjusted_end_date)\n",
        "plt.gca().xaxis.set_major_locator(mdates.YearLocator(2))\n",
        "plt.xlabel(\"Year\", fontsize=14, fontweight=\"bold\")\n",
        "plt.ylabel(\"Number of Circulars\", fontsize=14, fontweight=\"bold\")\n",
        "plt.title(\"Stacked Histogram of Observations Types over Time\", fontsize=18, fontweight=\"bold\", y=1.02)\n",
        "plt.grid(axis=\"both\", linestyle=\"-\", alpha=0.5)\n",
        "handles = [Patch(color=custom_colors[i], label=topic_model.topic_labels_[i]) for i in range(len(custom_colors))]\n",
        "plt.legend(handles=handles, bbox_to_anchor=(0.01, 0.99), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5edOPX82Nyc7",
        "dQHTzlMLn3wl",
        "4qXSkpOy36e7",
        "QqKXDs1FqgBb",
        "8nwZhk1A8OKt",
        "phnVmCY1HoBn"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}